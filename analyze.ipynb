{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\linh.nguyenkhanh3\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\linh.nguyenkhanh3\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    paragraphs = text.split('\\n')\n",
    "    \n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    paragraph_count = len([p for p in paragraphs if p.strip()])\n",
    "    \n",
    "    word_freq = Counter(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    vocab_v1 = set(word_freq.keys())\n",
    "    vocab_v2 = {word for word in vocab_v1 if word.lower() not in stop_words}\n",
    "    \n",
    "    unigrams = words\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    trigrams = list(nltk.trigrams(words))\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"paragraph_count\": paragraph_count,\n",
    "        \"vocab_v1\": vocab_v1,\n",
    "        \"vocab_v2\": vocab_v2,\n",
    "        \"unigrams\": unigrams,\n",
    "        \"bigrams\": bigrams,\n",
    "        \"trigrams\": trigrams,\n",
    "        \"named_entities\": named_entities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis(results):\n",
    "    print(f\"Total Words: {results['word_count']}\")\n",
    "    print(f\"Total Sentences: {results['sentence_count']}\")\n",
    "    print(f\"Total Paragraphs: {results['paragraph_count']}\")\n",
    "    print(\"Vocabulary V1:\", ', '.join(results['vocab_v1']))\n",
    "    print(\"Vocabulary V2 (After Stopword Removal):\", ', '.join(results['vocab_v2']))\n",
    "    print(\"Unigrams:\", ', '.join(results['unigrams']))\n",
    "    print(\"Bigrams:\", ', '.join([f\"{b[0]} {b[1]}\" for b in results['bigrams']]))\n",
    "    print(\"Trigrams:\", ', '.join([f\"{t[0]} {t[1]} {t[2]}\" for t in results['trigrams']]))\n",
    "    print(\"Named Entities:\")\n",
    "    for ent, label in results['named_entities']:\n",
    "        print(f\"{ent} ({label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đại học, Tin tuyển sinh\n",
      "Trường Đại học Phenikaa công bố 5 phương thức tuyển sinh đại học năm 2022\n",
      "Trường Đại học Phenikaa công bố thông tin tuyển sinh năm 2022, theo đó Trường tuyển sinh 4942 chỉ tiêu cho 36 ngành/chương trình đào tạo với 05 phương thức xét tuyển.\n",
      "1. Mã trường: PKA\n",
      "2. Chỉ tiêu tuyển sinh: 4942\n",
      "3. Phương thức tuyển sinh\n",
      "Trong quá trình triển khai, Nhà trường sẽ xem xét và điều chỉnh linh động tỷ lệ chỉ tiêu giữa các phương thức cho phù hợp với tình hình thực tế.\n",
      "4. Điều kiện xét tuyển\n",
      "Quy định xét tuyển (không áp dụng đối với phương thức xét tuyển theo kết quả thi tốt nghiệp THPT năm 2022):\n",
      "Trong đó:\n",
      "4.1. Phương thức 1: Xét tuyển thẳng\n",
      "Điều kiện xét tuyển: thí sinh đủ tiêu chuẩn công nhận tốt nghiệp THPT của Bộ Giáo dục và Đào tạo, đồng thời thuộc một trong những trường hợp dưới đây:\n",
      "4.1.1. Xét tuyển thẳng theo quy định của Bộ Giáo dục và Đào tạo\n",
      "a. Tham dự kỳ thi lựa chọn đội tuyển quốc gia dự cuộc thi Olympic quốc tế được xét tuyển thẳng vào ngành/chương trình đào tạo phù hợp với môn thi của thí sinh;\n",
      "b. Thành viên đội tuyển quốc gia dự thi Khoa học kỹ thuật (KHKT) quốc tế được xét tuyển thẳng vào ngành/chương trình đào tạo phù hợp với môn thi hoặc nội dung đề tài dự thi của thí sinh. Hội đồng tuyển sinh Trường Đại học Phenikaa xem xét và quyết định;\n",
      "c. Đạt giải Nhất/Nhì/Ba kỳ thi Học sinh giỏi (HSG) cấp quốc gia được xét tuyển thẳng vào các ngành/chương trình đào tạo có môn đạt giải nằm trong tổ hợp môn xét tuyển đối với ngành/chương trình đào tạo đăng ký. Riêng thí sinh đạt giải môn Tin học được tuyển thẳng vào tất cả các ngành/chương trình đào tạo.\n",
      "4.1.2. Xét tuyển thẳng theo Đề án tuyển sinh riêng của Trường Đại học Phenikaa\n",
      "a. Đối tượng 1: Đạt giải Khuyến khích kỳ thi HSG cấp tỉnh/thành phố trở lên được xét tuyển thẳng vào các ngành/chương trình đào tạo có môn đạt giải nằm trong tổ hợp xét tuyển đối với ngành học đăng ký. Riêng học sinh đạt giải môn Tin học được tuyển thẳng vào tất cả các ngành/chương trình đào tạo;\n",
      "b. Đối tượng 2: Học sinh hệ chuyên thuộc các trường THPT chuyên các tỉnh/thành phố hoặc các trường có lớp chuyên do UBND các tỉnh/thành phố công nhận có điểm tổ hợp xét tuyển đạt từ 24 điểm trở lên được đăng kí xét tuyển thẳng vào các ngành học có môn chuyên thuộc tổ hợp xét tuyển. Học sinh chuyên môn Tin học đạt điều kiện trên có thể đăng ký xét tuyển thẳng vào tất cả các ngành/chương trình đào tạo;\n",
      "c. Đối tượng 3: Học sinh không thuộc hệ chuyên có điểm trung bình các môn học 3 học kỳ đạt 8,0 trở lên, đồng thời có điểm trung bình mỗi môn học trong tổ hợp xét tuyển đạt từ 8,5 trở lên;\n",
      "d. Đối tượng 4: Học sinh có 1 trong các chứng chỉ sau đây:\n",
      "Lưu ý: Các chứng chỉ quốc tế phải còn thời hạn sử dụng tính đến 30/6/2022.\n",
      "e. Đối tượng 5: Học sinh các trường THPT liên kết/hợp tác với Trường Đại học Phenikaa thỏa mãn điều kiện sau:\n",
      "f. Đối tượng 6: Học sinh thuộc diện gia đình chính sách, hộ nghèo, cận nghèo, khuyết tật theo quy định của Thủ tướng Chính phủ và có điểm trung bình các môn 3 học kỳ đạt từ 8,0 trở lên;\n",
      "g. Đối tượng 7: Có bằng đại học hệ chính quy từ loại Khá trở lên;\n",
      "h. Đối tượng 8: Người Việt Nam tốt nghiệp THPT ở nước ngoài hoặc các trường quốc tế tại Việt Nam có chứng nhận văn bằng của cơ quan có thẩm quyền;\n",
      "i. Đối tượng 9: Áp dụng riêng đối với 2 ngành/chương trình đào tạo tài năng\n",
      "i1) Đối với ngành Vật lý tài năng:\n",
      "i2) Đối với ngành Khoa học máy tính tài năng:\n",
      "Lưu ý chung đối với phương thức xét tuyển thẳng:\n",
      "Trường hợp thí sinh có điểm xét tuyển bằng nhau: ưu tiên thí sinh có thứ tự nguyện vọng cao hơn, thời gian nộp hồ sơ sớm hơn;\n",
      "4.2. Phương thức 2: Xét tuyển dựa vào kết quả kỳ thi tốt nghiệp THPT năm 2022\n",
      "Điểm xét tuyển là tổng điểm theo tổ hợp 3 môn xét tuyển cộng điểm ưu tiên đối tượng và khu vực theo quy định của Bộ Giáo dục và Đào tạo, cộng điểm chứng chỉ ngoại ngữ theo quy định của Trường Đại học Phenikaa (Bảng 1);\n",
      "Đối với khối ngành Sức khỏe phải đạt ngưỡng đảm bảo chất lượng của Bộ GD&ĐT.\n",
      "Tiêu chí xét tuyển: Điểm xét tuyển lấy từ cao xuống thấp. Trường hợp thí sinh có điểm xét tuyển bằng nhau thì ưu tiên thí sinh có nguyện vọng (NV) cao hơn.\n",
      "4.3. Phương thức 3:  Xét tuyển dựa vào kết quả học tập bậc THPT\n",
      "Điểm sàn nhận hồ sơ xét tuyển: Điểm tổ hợp xét tuyển đạt mức điểm theo quy định như sau:\n",
      "Đối với nhóm ngành ngôn ngữ (Ngôn ngữ Anh, Ngôn ngữ Hàn Quốc, Ngôn ngữ Trung Quốc, Ngôn ngữ Nhật Bản) cần thêm điều kiện điểm trung bình môn ngoại ngữ (tiếng Anh, tiếng Trung Quốc, tiếng Hàn Quốc, tiếng Nhật) đạt từ 6,5 trở lên.\n",
      "Tiêu chí xét tuyển: Điểm xét tuyển lấy từ cao xuống thấp. Trường hợp thí sinh có điểm xét tuyển bằng nhau thì ưu tiên thí sinh có nguyện vọng (NV) cao hơn, nộp hồ sơ xét tuyển sớm hơn.\n",
      "4.4. Phương thức 4: Xét tuyển dựa vào kết quả học tập bậc THPT kết hợp với phỏng vấn\n",
      "Áp dụng với các ngành: Kỹ thuật robot và trí tuệ nhân tạo (Đào tạo song ngữ Việt- Anh), Khoa học máy tính (đào tạo tài năng), Kỹ thuật phục hồi chức năng, Kỹ thuật xét nghiệm y học, Y khoa.\n",
      "Điều kiện xét tuyển: thí sinh đủ tiêu chuẩn công nhận tốt nghiệp THPT của Bộ Giáo dục và Đào tạo, đồng thời có điểm tổ hợp xét tuyển đạt từ 21 điểm trở lên.\n",
      "Khối ngành Sức khỏe (áp dụng với ngành Kỹ thuật phục hồi chức năng, Kỹ thuật xét nghiệm y học, Y khoa). Thí sinh đăng ký xét tuyển nhóm ngành sức khỏe phải đạt ngưỡng đảm bảo chất lượng theo quy định chung của Bộ GD&ĐT.\n",
      "Tiêu chí xét tuyển: Chấm điểm hồ sơ đánh giá năng lực theo mẫu quy định của Trường Đại học Phenikaa. Điểm xét tuyển lấy từ cao xuống thấp. Trường hợp thí sinh có điểm xét tuyển bằng nhau thì ưu tiên thí sinh có nguyện vọng (NV) cao hơn, nộp hồ sơ xét tuyển sớm hơn.\n",
      "4.5. Phương thức 5: Xét tuyển dựa vào kết quả thi đánh giá tư duy của Trường Đại học Bách khoa Hà Nội và kết quả thi đánh giá năng lực (ĐGNL) của Đại học Quốc gia Hà Nội.\n",
      "Điều kiện xét tuyển: thí sinh đủ tiêu chuẩn công nhận tốt nghiệp THPT của Bộ Giáo dục và Đào tạo, đồng thời đạt 1 trong 2 tiêu chí:\n",
      "5. Phương thức cộng điểm đối với chứng chỉ ngoại ngữ quốc tế\n",
      "Với các thí sinh có chứng chỉ tiếng Anh quốc tế (IELTS, TOEFL iBT, PTE Academic, và Cambridge) tương đương với IELTS từ 4.0 trở lên; chứng chỉ tiếng Trung Quốc từ HSK3 trở lên; chứng chỉ tiếng Hàn Quốc từ TOPIK3 trở lên; chứng chỉ tiếng Nhật từ N5 (JLBT) trở lên, Trường Đại học Phenikaa sẽ xét cộng điểm ưu tiên vào tổng điểm xét tuyển cho tất cả các ngành/chương trình đào tạo đăng ký xét tuyển với các mức như trong Bảng 1.\n",
      "6. Chỉ tiêu tuyển sinh, tổ hợp môn xét tuyển và học phí\n",
      "7. Quỹ học bổng và hỗ trợ học phí\n",
      "Quỹ học bổng dành cho sinh viên trúng tuyển đại học chính quy năm 2022 vào Trường Đại học Phenikaa được thành lập từ nhiều nguồn: Tài trợ từ Tập đoàn Phenikaa; tài trợ từ các công ty thành viên của Tập đoàn Phenikaa; tài trợ từ các doanh nghiệp, đối tác; tài trợ từ các cựu sinh viên thành đạt và các nguồn hợp pháp khác với tổng giá trị trên 50 tỷ đồng.\n",
      "7.1. Chính sách học bổng\n",
      "Áp dụng với thí sinh nhập học đợt 1 của các phương thức xét tuyển và nộp đầy đủ giấy tờ về Trường theo đúng quy định.\n",
      "7.1.1. Học bổng Chairman’s scholarship (trị giá 170 – 300 triệu đồng) – Miễn học phí toàn khóa học đồng thời nhận tài trợ chi phí sinh hoạt trị giá 20 triệu đồng/năm\n",
      "Năm 2022, Trường Đại học Phenikaa chọn trao 10 suất học bổng đặc biệt dành cho các thí sinh đăng ký và nhập học tại Trường Đại học Phenikaa, đáp ứng được 1 trong các tiêu chí sau:\n",
      "Tiêu chí nhận học bổng:\n",
      "Lưu ý: Điều kiện duy trì học bổng cho những năm tiếp theo là năm trước đó đạt kết quả học tập cả năm từ 7,5 trở lên (thang điểm 10).\n",
      "7.1.2. Học bổng Tài năng – Miễn học phí toàn khóa học (trị giá 80 – 170 triệu đồng)\n",
      "Riêng các ngành/chương trình đào tạo tài năng (Vật lý tài năng, Khoa học máy tính tài năng)\n",
      "Miễn toàn bộ học phí trong 4 năm cho 10 em sinh viên xuất sắc nhất của mỗi ngành, khóa 2022-2026.\n",
      "Tiêu chí nhận học bổng:\n",
      "Lưu ý: Điều kiện duy trì học bổng cho những năm tiếp theo là năm trước đó đạt kết quả học tập cả năm từ 7,5 trở lên (thang điểm 10).\n",
      "7.1.3. Học bổng Xuất sắc – Miễn học phí 2 năm đầu tiên (trị giá 40 – 80 triệu đồng)\n",
      "Lưu ý: Điều kiện duy trì học bổng cho những năm tiếp theo là năm trước đó đạt kết quả học tập cả năm từ 7,5 trở lên (thang điểm 10).\n",
      "7.1.4. Học bổng Chắp cánh tương lai – Miễn học phí năm đầu tiên (trị giá 20-40 triệu đồng)\n",
      "Lưu ý: Trong quá trình xét tuyển, khi có những trường hợp đặc biệt, Phòng TSTT đề xuất, Hội đồng tuyển sinh Trường sẽ quyết định.\n",
      "7.2. Chính sách hỗ trợ học phí\n",
      "7.3. Chính sách hỗ trợ khác\n",
      "Chi tiết liên hệ Phòng Tuyển sinh và Truyền thông, Trường Đại học Phenikaa:\n",
      "\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\linh.nguyenkhanh3/nltk_data'\n    - 'd:\\\\Data Science\\\\.venv\\\\nltk_data'\n    - 'd:\\\\Data Science\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Data Science\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\linh.nguyenkhanh3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     text_content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(text_content)\n\u001b[1;32m----> 7\u001b[0m print_analysis(\u001b[43manalyze_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m, in \u001b[0;36manalyze_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_text\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m----> 3\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m      5\u001b[0m     paragraphs \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Data Science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Data Science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32md:\\Data Science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data Science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data Science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32md:\\Data Science\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\linh.nguyenkhanh3/nltk_data'\n    - 'd:\\\\Data Science\\\\.venv\\\\nltk_data'\n    - 'd:\\\\Data Science\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Data Science\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\linh.nguyenkhanh3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "data_path = \"crawled/data_tuyen_sinh_phenikaa.txt\"\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_content = file.read()\n",
    "    print(text_content)\n",
    "\n",
    "print_analysis(analyze_text(text_content))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
